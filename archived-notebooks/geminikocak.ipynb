{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "60bc54ff",
      "metadata": {
        "id": "60bc54ff"
      },
      "source": [
        "# Retrieve data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "93132d20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 29494,
          "status": "ok",
          "timestamp": 1747242457367,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "93132d20"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    is_running_on_colab = True\n",
        "except ImportError:\n",
        "    is_running_on_colab = False\n",
        "\n",
        "if is_running_on_colab:\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "268fe8d5",
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1747242457369,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "268fe8d5"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_notebook = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c29e4220",
      "metadata": {
        "executionInfo": {
          "elapsed": 1397,
          "status": "ok",
          "timestamp": 1747242458767,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "c29e4220"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\__init__.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspecial\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     19\u001b[39m _NUMPY_NAMESPACE_NAMES = {\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marray_api_compat.numpy\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34myield_namespaces\u001b[39m(include_numpy_namespaces=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\fixes.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\scipy\\stats\\__init__.py:624\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    619\u001b[39m \n\u001b[32m    620\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[32m    623\u001b[39m                                DegenerateDataWarning, FitError)\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Import unused here but needs to stay until end of deprecation periode\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# See https://github.com/scipy/scipy/issues/15765#issuecomment-1875564522\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linalg  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _mstats_basic \u001b[38;5;28;01mas\u001b[39;00m mstats_basic\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_mstats_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _find_repeats, theilslopes, siegelslopes\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\scipy\\stats\\distributions.py:10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_distn_infrastructure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_continuous_distns\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:30\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tukeylambda_stats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (tukeylambda_variance \u001b[38;5;28;01mas\u001b[39;00m _tlvar,\n\u001b[32m     26\u001b[39m                                  tukeylambda_kurtosis \u001b[38;5;28;01mas\u001b[39;00m _tlkurt)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_distn_infrastructure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (_vectorize_rvs_over_shapes,\n\u001b[32m     28\u001b[39m     get_distribution_names, _kurtosis, _isintegral,\n\u001b[32m     29\u001b[39m     rv_continuous, _skew, _get_fixed_fit_value, _check_shape, _ShapeInfo)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_distribution_infrastructure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _log1mexp\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ksstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m kolmogn, kolmognp, kolmogni\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_constants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (_XMIN, _LOGXMIN, _EULER, _ZETA3, _SQRT_PI,\n\u001b[32m     33\u001b[39m                          _SQRT_2_OVER_PI, _LOG_PI, _LOG_SQRT_2_OVER_PI)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\scipy\\stats\\_distribution_infrastructure.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chandrupatla\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _chandrupatla, _chandrupatla_minimize\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_probability_distribution\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _ProbabilityDistribution\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m qmc\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# in case we need to distinguish between None and not specified\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Typically this is used to determine whether the tolerance has been set by the\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# user and make a decision about which method to use to evaluate a distribution\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# be refined in follow-up work.\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# See https://github.com/scipy/scipy/pull/21050#discussion_r1714195433.\u001b[39;00m\n\u001b[32m     26\u001b[39m _null = \u001b[38;5;28mobject\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\scipy\\stats\\qmc.py:235\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m====================================================\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mQuasi-Monte Carlo submodule (:mod:`scipy.stats.qmc`)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m \n\u001b[32m    234\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_qmc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_qmc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __all__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\scipy\\stats\\_qmc.py:33\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gammainc\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sobol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     _initialize_v, _cscramble, _fill_p_cumulative, _draw, _fast_forward,\n\u001b[32m     31\u001b[39m     _categorize, _MAXDIM\n\u001b[32m     32\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_qmc_cy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     34\u001b[39m     _cy_wrapper_centered_discrepancy,\n\u001b[32m     35\u001b[39m     _cy_wrapper_wrap_around_discrepancy,\n\u001b[32m     36\u001b[39m     _cy_wrapper_mixture_discrepancy,\n\u001b[32m     37\u001b[39m     _cy_wrapper_l2_star_discrepancy,\n\u001b[32m     38\u001b[39m     _cy_wrapper_update_discrepancy,\n\u001b[32m     39\u001b[39m     _cy_van_der_corput_scrambled,\n\u001b[32m     40\u001b[39m     _cy_van_der_corput,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     44\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdiscrepancy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgeometric_discrepancy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mupdate_discrepancy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     45\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mQMCEngine\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSobol\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHalton\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLatinHypercube\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPoissonDisk\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     46\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mMultinomialQMC\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMultivariateNormalQMC\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     49\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_random_state\u001b[39m(seed: IntNumber | \u001b[38;5;28;01mNone\u001b[39;00m = ...) -> np.random.Generator:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:645\u001b[39m, in \u001b[36mparent\u001b[39m\u001b[34m(self)\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3a3ZranOYGrW",
      "metadata": {
        "executionInfo": {
          "elapsed": 8,
          "status": "ok",
          "timestamp": 1747242458777,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "3a3ZranOYGrW"
      },
      "outputs": [],
      "source": [
        "# Option to generate mock data\n",
        "generate_mock_data = False # Set to True to generate mock data\n",
        "max_mock_depth_ft = 2000 # Define maximum depth for mock data (if used)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e7b97178",
      "metadata": {
        "executionInfo": {
          "elapsed": 4631,
          "status": "ok",
          "timestamp": 1747242463409,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "e7b97178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from: ./dataset/solar_system_positions_with_velocity.csv\n",
            "Data loaded successfully. Shape: (63945, 9)\n",
            "Data head:\n",
            "          date                  name  naif_id      x_au      y_au      z_au  \\\n",
            "0  2020-01-01  1 MERCURY BARYCENTER        1 -0.063377 -0.460841 -0.031843   \n",
            "1  2020-01-02  1 MERCURY BARYCENTER        1 -0.041067 -0.462569 -0.034031   \n",
            "2  2020-01-03  1 MERCURY BARYCENTER        1 -0.018637 -0.462941 -0.036119   \n",
            "3  2020-01-04  1 MERCURY BARYCENTER        1  0.003848 -0.461948 -0.038101   \n",
            "4  2020-01-05  1 MERCURY BARYCENTER        1  0.026321 -0.459583 -0.039969   \n",
            "\n",
            "   vx_au_per_day  vy_au_per_day  vz_au_per_day  \n",
            "0       0.022228      -0.002402      -0.002235  \n",
            "1       0.022381      -0.001052      -0.002139  \n",
            "2       0.022468       0.000309      -0.002036  \n",
            "3       0.022490       0.001678      -0.001926  \n",
            "4       0.022445       0.003053      -0.001810  \n"
          ]
        }
      ],
      "source": [
        "# well_name = \"LLB-10\" # Original well name, no longer used with the new dataset\n",
        "if not generate_mock_data:\n",
        "  if is_running_on_colab:\n",
        "    # Load data from Google Drive, if running on Google Colab\n",
        "    # IMPORTANT: Update this path if solar_system_positions_with_velocity.csv is in a different location\n",
        "    colab_repo_dir = \"/content/drive/MyDrive/riset-fttm-gdrive/cuml-tf-model-hydrocarbon-prediction\"\n",
        "    data_path = f\"{colab_repo_dir}/data/solar_system_positions_with_velocity.csv\"\n",
        "    # If the file is uploaded directly to Colab session, use its direct path e.g., \"/content/solar_system_positions_with_velocity.csv\"\n",
        "    # data_path = \"/content/solar_system_positions_with_velocity.csv\" \n",
        "    print(f\"Loading data from: {data_path}\")\n",
        "    data = pd.read_csv(data_path, sep=',')\n",
        "  else:\n",
        "    # Load data from local directory\n",
        "    # IMPORTANT: Ensure solar_system_positions_with_velocity.csv is in this path or update it\n",
        "    local_data_path = \"./dataset/solar_system_positions_with_velocity.csv\"\n",
        "    print(f\"Loading data from: {local_data_path}\")\n",
        "    data = pd.read_csv(local_data_path, sep=',')\n",
        "  print(\"Data loaded successfully. Shape:\", data.shape)\n",
        "  print(\"Data head:\\n\", data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ff2fd574",
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1747242463409,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "ff2fd574"
      },
      "outputs": [],
      "source": [
        "if generate_mock_data:\n",
        "    # This section is for generating mock data if the primary data source is not used.\n",
        "    # It's retained for flexibility but won't run if generate_mock_data is False.\n",
        "    print(f\"Generating mock data up to {max_mock_depth_ft} ft...\") # Adjusted print message\n",
        "    mock_depth_step = 0.5\n",
        "    mock_y_values = np.arange(0, max_mock_depth_ft, mock_depth_step) # Changed DEPT to y_values for clarity\n",
        "    num_mock_rows = len(mock_y_values)\n",
        "\n",
        "    mock_data_dict = {'Y_AXIS_MOCK': mock_y_values} # Using a generic y-axis name\n",
        "\n",
        "    # Using generic feature names for mock data, as original features are not relevant\n",
        "    feature_cols_for_mock = [f'MockFeature_{i}' for i in range(8)] \n",
        "\n",
        "    for col in feature_cols_for_mock:\n",
        "        mock_data_dict[col] = np.random.rand(num_mock_rows) * 100\n",
        "\n",
        "    # Assuming a binary classification task for mock data, as in the original notebook\n",
        "    mock_data_dict['mock_target_class'] = np.random.randint(0, 2, num_mock_rows)\n",
        "\n",
        "    data = pd.DataFrame(mock_data_dict)\n",
        "\n",
        "    print(f\"Mock data generated with {num_mock_rows} rows and columns: {list(data.columns)}.\")\n",
        "    print(\"Mock data head:\")\n",
        "    print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "689d083c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "executionInfo": {
          "elapsed": 100,
          "status": "ok",
          "timestamp": 1747242463508,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "689d083c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original data shape before dropping NaNs from features: (63945, 9)\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "['x', 'y', 'z', 'vx', 'vy', 'vz']",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_2736\\881818268.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Drop rows with NaN in feature_cols to avoid issues with scalers/models\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m generate_mock_data: \u001b[38;5;66;03m# Only apply if not using mock data\u001b[39;00m\n\u001b[32m     15\u001b[39m     print(f\"Original data shape before dropping NaNs from features: {data.shape}\")\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     data.dropna(subset=feature_cols, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     17\u001b[39m     print(f\"Data shape after dropping NaNs from features: {data.shape}\")\n\u001b[32m     18\u001b[39m     df = data[feature_cols].copy() \u001b[38;5;66;03m# Use .copy() to avoid SettingWithCopyWarning\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# If using mock data, df is based on mock_data_dict feature_cols_for_mock\u001b[39;00m\n",
            "\u001b[32mc:\\ARF\\my-repos\\RBL-AnalisisData-ML\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[39m\n\u001b[32m   6666\u001b[39m             ax = self._get_axis(agg_axis)\n\u001b[32m   6667\u001b[39m             indices = ax.get_indexer_for(subset)\n\u001b[32m   6668\u001b[39m             check = indices == -\u001b[32m1\u001b[39m\n\u001b[32m   6669\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m check.any():\n\u001b[32m-> \u001b[39m\u001b[32m6670\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m KeyError(np.array(subset)[check].tolist())\n\u001b[32m   6671\u001b[39m             agg_obj = self.take(indices, axis=agg_axis)\n\u001b[32m   6672\u001b[39m \n\u001b[32m   6673\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m lib.no_default:\n",
            "\u001b[31mKeyError\u001b[39m: ['x', 'y', 'z', 'vx', 'vy', 'vz']"
          ]
        }
      ],
      "source": [
        "# df=data[['CALI','DRHO','GR','MR','NPHI_corr','PEF','RHOB_CORR','ROP']] # Original features\n",
        "# TODO: Update feature_cols based on the columns in solar_system_positions_with_velocity.csv\n",
        "# Assuming columns like 'x', 'y', 'z', 'vx', 'vy', 'vz' are relevant numerical features.\n",
        "# Please inspect your CSV and adjust these column names accordingly.\n",
        "# For example, if your CSV has these columns:\n",
        "feature_cols = ['x', 'y', 'z', 'vx', 'vy', 'vz'] # Example feature columns\n",
        "\n",
        "# Ensure these columns exist in your 'data' DataFrame and are numeric.\n",
        "# Handle non-numeric columns (e.g., 'Object', 'Date', 'Time') appropriately if they are part of feature_cols\n",
        "# For instance, 'Object' might need encoding if used as a feature.\n",
        "# 'Date' and 'Time' might need conversion to numerical representations (e.g., timestamps, cyclical features).\n",
        "\n",
        "# Drop rows with NaN in feature_cols to avoid issues with scalers/models\n",
        "if not generate_mock_data: # Only apply if not using mock data\n",
        "    print(f\"Original data shape before dropping NaNs from features: {data.shape}\")\n",
        "    data.dropna(subset=feature_cols, inplace=True)\n",
        "    print(f\"Data shape after dropping NaNs from features: {data.shape}\")\n",
        "    df = data[feature_cols].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "else: # If using mock data, df is based on mock_data_dict feature_cols_for_mock\n",
        "    df = data[[col for col in data.columns if col not in ['Y_AXIS_MOCK', 'mock_target_class']]].copy()\n",
        "\n",
        "print(\"Selected features (df) head:\\n\", df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c353b40e",
      "metadata": {
        "id": "c353b40e"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "804b8005",
      "metadata": {
        "id": "804b8005"
      },
      "source": [
        "## Train/Test Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39bfef38",
      "metadata": {
        "executionInfo": {
          "elapsed": 7,
          "status": "ok",
          "timestamp": 1747242463510,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "39bfef38"
      },
      "outputs": [],
      "source": [
        "# Misalkan 'data' adalah DataFrame Anda dan 'df' adalah fitur yang telah Anda ekstrak\n",
        "X = df  # Fitur\n",
        "# y = data['hydrocarbon_formation_class']  # Original Label\n",
        "\n",
        "# TODO: Define your target variable 'y' based on the new dataset.\n",
        "# For example, if 'Object' is a categorical target in your solar_system_positions_with_velocity.csv:\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# label_encoder = LabelEncoder()\n",
        "# y = pd.Series(label_encoder.fit_transform(data['Object']), index=X.index) # Ensure index alignment with X\n",
        "# Or if you have another target column:\n",
        "# y = data['your_target_column_name'].loc[X.index] # Ensure index alignment\n",
        "\n",
        "# For now, y, y_train, y_test will be undefined or based on mock data if generate_mock_data is True.\n",
        "# Model training and evaluation cells below will need to be adapted once 'y' is defined for the solar system data.\n",
        "if generate_mock_data:\n",
        "    y = data['mock_target_class']\n",
        "else:\n",
        "    y = None # Placeholder, user needs to define this for the solar system data\n",
        "\n",
        "if y is not None:\n",
        "    # Ensure X and y have the same index before splitting\n",
        "    common_index = X.index.intersection(y.index)\n",
        "    X = X.loc[common_index]\n",
        "    y = y.loc[common_index]\n",
        "    \n",
        "    # Split data menjadi training dan testing set\n",
        "    # Removed stratify=y as y might not be suitable for stratification or is undefined for the new data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "else:\n",
        "    # If y is not defined, split only X for unsupervised tasks or if y will be defined later\n",
        "    X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
        "    y_train, y_test = None, None # Explicitly set to None\n",
        "    print(\"Target variable 'y' is not defined for the solar system data. y_train and y_test are None.\")\n",
        "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "    print(\"Model training and evaluation cells requiring 'y' will need adaptation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "089ac779",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "executionInfo": {
          "elapsed": 26,
          "status": "ok",
          "timestamp": 1747242463535,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "089ac779"
      },
      "outputs": [],
      "source": [
        "if X_train is not None:\n",
        "    print(\"X_train description:\")\n",
        "    print(X_train.describe())\n",
        "else:\n",
        "    print(\"X_train is None.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f23bd7ae",
      "metadata": {},
      "source": [
        "## Data Imbalance Handling (SMOTE - Commented out as target 'y' is not defined for solar system data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733bd04c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SVMSMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ac8e952",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is commented out because SMOTE requires a defined 'y_train' for classification.\n",
        "# If you define 'y_train' for a classification task with the solar system data and it's imbalanced,\n",
        "# you can uncomment and adapt this cell.\n",
        "\n",
        "# print(\"SMOTE section (commented out):\")\n",
        "# if y_train is not None and X_train is not None and not y_train.empty:\n",
        "#     # Initialize SVMSMOTE\n",
        "#     smote = SVMSMOTE(random_state=42)\n",
        "# \n",
        "#     # Apply SMOTE to the training data\n",
        "#     print(\"Original training data shape:\", X_train.shape, y_train.shape)\n",
        "#     print(\"Original training class distribution:\\n\", y_train.value_counts())\n",
        "# \n",
        "#     try:\n",
        "#         X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "# \n",
        "#         print(\"\\nShape of training data after SMOTE:\", X_train_smote.shape, y_train_smote.shape)\n",
        "#         print(\"Class distribution after SMOTE:\\n\", y_train_smote.value_counts())\n",
        "# \n",
        "#         # Update X_train and y_train to be the oversampled versions\n",
        "#         X_train = X_train_smote\n",
        "#         y_train = y_train_smote\n",
        "#     except ValueError as e:\n",
        "#         print(f\"\\nSMOTE could not be applied: {e}\")\n",
        "#         print(\"This might happen if a class has too few samples for SVMSMOTE.\")\n",
        "#         print(\"Original X_train and y_train will be used.\")\n",
        "# else:\n",
        "#     print(\"\\nSMOTE was not applied as y_train or X_train is None or y_train is empty.\")\n",
        "#     print(\"Original X_train and y_train (if defined) will be used.\")\n",
        "print(\"SMOTE is commented out as 'y_train' is likely undefined for the solar system data or task is not classification.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fdf19b",
      "metadata": {
        "id": "33fdf19b"
      },
      "source": [
        "## Apply Quantile Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f4204b",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1747242463538,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "58f4204b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import QuantileTransformer\n",
        "def transform_quantile(X_train_in, X_test_in, X_in):\n",
        "    qt_transformer = QuantileTransformer(output_distribution='normal', random_state=42) # Added random_state for reproducibility\n",
        "    # Ensure inputs are not None before proceeding\n",
        "    if X_train_in is None or X_test_in is None or X_in is None:\n",
        "        print(\"Quantile transformation skipped as one or more input DataFrames (X_train, X_test, X) are None.\")\n",
        "        return X_train_in, X_test_in, X_in\n",
        "        \n",
        "    # Ensure inputs are not empty\n",
        "    if X_train_in.empty or X_test_in.empty or X_in.empty:\n",
        "        print(\"Quantile transformation skipped as one or more input DataFrames are empty.\")\n",
        "        return X_train_in, X_test_in, X_in\n",
        "\n",
        "    X_train_qt = pd.DataFrame(qt_transformer.fit_transform(X_train_in), columns=X_train_in.columns, index=X_train_in.index)\n",
        "    X_test_qt = pd.DataFrame(qt_transformer.transform(X_test_in), columns=X_test_in.columns, index=X_test_in.index)\n",
        "    X_qt = pd.DataFrame(qt_transformer.transform(X_in), columns=X_in.columns, index=X_in.index) # Transform the whole X for consistency if needed elsewhere\n",
        "    \n",
        "    return X_train_qt, X_test_qt, X_qt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7681d51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 87,
          "status": "ok",
          "timestamp": 1747242463627,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "b7681d51"
      },
      "outputs": [],
      "source": [
        "if X_train is not None and X_test is not None and X is not None:\n",
        "    X_train, X_test, X = transform_quantile(X_train, X_test, X)\n",
        "    print(\"Quantile transformation applied.\")\n",
        "else:\n",
        "    print(\"Skipping quantile transformation as X_train, X_test, or X is None.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32c2b3d1",
      "metadata": {
        "id": "32c2b3d1"
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "karena menggunakan Quatile transformation dengan output gaussian, masing masing kolom secara otomatis ditransformasi ke distribusi normal baku, atau distribusi normal dengan rataan nol dan standar deviasi 1, oleh karena itu tidak diperlukan tambahan scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6deec4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "executionInfo": {
          "elapsed": 35,
          "status": "ok",
          "timestamp": 1747242463660,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "ae6deec4"
      },
      "outputs": [],
      "source": [
        "if X_train is not None:\n",
        "    print(\"X_train description after quantile transformation:\")\n",
        "    print(X_train.describe())\n",
        "else:\n",
        "    print(\"X_train is None, cannot describe.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1811d39",
      "metadata": {
        "id": "e1811d39"
      },
      "source": [
        "# Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de585b43",
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1747242463664,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "de585b43"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "sk_train_accuracy={}\n",
        "sk_test_accuracy={}\n",
        "\n",
        "cu_train_accuracy={}\n",
        "cu_test_accuracy={}\n",
        "\n",
        "sk_crossValidation_accuracy={}\n",
        "cu_crossValidation_accuracy={}\n",
        "\n",
        "sk_models = {} #sklearn models\n",
        "cu_models = {} #cuml models\n",
        "\n",
        "sk_times = {}\n",
        "cu_times = {}\n",
        "\n",
        "sk_pred = {}\n",
        "cu_pred = {}\n",
        "\n",
        "sk_pred_times = {}\n",
        "cu_pred_times = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02573183",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 12770,
          "status": "ok",
          "timestamp": 1747242476436,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "02573183"
      },
      "outputs": [],
      "source": [
        "# test CuML availability & is working\n",
        "try:\n",
        "    import cuml\n",
        "    # Check if X_train is not None and not empty before creating dummy data for KMeans\n",
        "    if X_train is not None and not X_train.empty:\n",
        "        # Create minimal dummy data based on X_train's structure if possible, or generic\n",
        "        if X_train.shape[1] >= 2:\n",
        "            dummy_data_np = X_train.iloc[:4, :2].to_numpy() # Use a small slice of actual data\n",
        "        else: # Fallback if X_train has fewer than 2 columns\n",
        "            dummy_data_np = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8]], dtype=np.float32)\n",
        "        kmeans = cuml.KMeans(n_clusters=2, random_state=42)\n",
        "        kmeans.fit(dummy_data_np)\n",
        "        has_cuml = True\n",
        "        print(\"cuML is found and working\")\n",
        "    else:\n",
        "        has_cuml = False\n",
        "        print(\"X_train is None or empty, cannot perform cuML KMeans test. Assuming cuML is not fully usable.\")\n",
        "except ImportError:\n",
        "    has_cuml = False\n",
        "    print(\"cuML not found. Please ensure cuML is installed.\")\n",
        "except Exception as e:\n",
        "    has_cuml = False\n",
        "    print(f\"cuML couldn't be initialized or used. Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4093261",
      "metadata": {
        "id": "b4093261"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1136f4d",
      "metadata": {
        "id": "b1136f4d"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb0c14c7",
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1747242476437,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "cb0c14c7"
      },
      "outputs": [],
      "source": [
        "model_name = \"SVM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4016de5c",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1747242476437,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "4016de5c"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC as SklearnSVC\n",
        "# Attempt to import cuML's SVC\n",
        "try:\n",
        "    from cuml.svm import SVC as cuMLSVC\n",
        "    cuml_svc_available = True\n",
        "except ImportError:\n",
        "    cuml_svc_available = False\n",
        "    print(\"cuML SVC not available. Please ensure cuML is installed and compatible with your environment.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c77488",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 8903,
          "status": "ok",
          "timestamp": 1747242485338,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "b9c77488"
      },
      "outputs": [],
      "source": [
        "# Parameter grid for both models\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'random_state': [42] # Added for reproducibility\n",
        "}\n",
        "\n",
        "# 1) scikit-learn SVM with GridSearchCV\n",
        "sk_models[model_name] = GridSearchCV(\n",
        "    estimator=SklearnSVC(kernel='rbf', probability=True), # probability=True for predict_proba if needed later\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    verbose=1, # Reduced verbosity for cleaner output\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "time_start_sk = time.time()\n",
        "if X_train is not None and y_train is not None:\n",
        "    print(f\"Fitting scikit-learn {model_name}...\")\n",
        "    sk_models[model_name].fit(X_train, y_train)\n",
        "    sk_times[model_name] = time.time() - time_start_sk\n",
        "    print(f\"scikit-learn GridSearchCV training time ({model_name}) : {sk_times[model_name]:.2f} seconds\")\n",
        "    print(f\"scikit-learn Best parameters ({model_name}): {sk_models[model_name].best_params_}\")\n",
        "else:\n",
        "    sk_times[model_name] = 0\n",
        "    print(f\"Skipping scikit-learn {model_name} fitting as X_train or y_train is not defined.\")\n",
        "\n",
        "# 2) cuML SVM with the same GridSearchCV\n",
        "if has_cuml and cuml_svc_available:\n",
        "    cu_models[model_name] = GridSearchCV(\n",
        "        estimator=cuMLSVC(kernel='rbf', probability=True, random_state=42),\n",
        "        param_grid=param_grid,\n",
        "        cv=5,\n",
        "        verbose=1,\n",
        "        n_jobs=1 \n",
        "    )\n",
        "\n",
        "    time_start_cu = time.time()\n",
        "    if X_train is not None and y_train is not None:\n",
        "        print(f\"Fitting cuML {model_name}...\")\n",
        "        cu_models[model_name].fit(X_train, y_train)\n",
        "        cu_times[model_name] = time.time() - time_start_cu\n",
        "        print(f\"cuml GridSearchCV training time ({model_name}) : {cu_times[model_name]:.2f} seconds\")\n",
        "        print(f\"cuml Best parameters ({model_name}): {cu_models[model_name].best_params_}\")\n",
        "    else:\n",
        "        cu_times[model_name] = 0\n",
        "        print(f\"Skipping cuML {model_name} fitting as X_train or y_train is not defined.\")\n",
        "elif has_cuml and not cuml_svc_available:\n",
        "    print(f\"cuML is available, but cuML {model_name} (SVC) is not. Skipping cuML {model_name} benchmark.\")\n",
        "else:\n",
        "    print(f\"cuML is not installed or GPU not available. Skipping cuML {model_name} benchmark.\")\n",
        "\n",
        "if X_train is None or y_train is None: # Print this only if fitting was skipped for sklearn\n",
        "   print(f\"scikit-learn GridSearchCV training time ({model_name}) : N/A (X_train or y_train not defined)\")\n",
        "   print(f\"scikit-learn Best parameters ({model_name}): N/A (X_train or y_train not defined)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9e4ebd3",
      "metadata": {
        "id": "f9e4ebd3"
      },
      "source": [
        "## K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b59478",
      "metadata": {
        "executionInfo": {
          "elapsed": 11,
          "status": "ok",
          "timestamp": 1747242485349,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "d4b59478"
      },
      "outputs": [],
      "source": [
        "model_name = \"KNN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "307602ed",
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1747242485351,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "307602ed"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier as SklearnKNeighborsClassifier\n",
        "# Attempt to import cuML's KNeighborsClassifier\n",
        "try:\n",
        "    from cuml.neighbors import KNeighborsClassifier as cuMLKNeighborsClassifier\n",
        "    cuml_knn_available = True\n",
        "except ImportError:\n",
        "    cuml_knn_available = False\n",
        "    print(\"cuML KNeighborsClassifier not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921e7503",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3458,
          "status": "ok",
          "timestamp": 1747242488815,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "921e7503"
      },
      "outputs": [],
      "source": [
        "# Parameter grid for KNN\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# 1) scikit-learn KNN with GridSearchCV\n",
        "sk_models[model_name] = GridSearchCV(\n",
        "    estimator=SklearnKNeighborsClassifier(),\n",
        "    param_grid=param_grid_knn,\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "time_start_sk = time.time()\n",
        "if X_train is not None and y_train is not None:\n",
        "    print(f\"Fitting scikit-learn {model_name}...\")\n",
        "    sk_models[model_name].fit(X_train, y_train)\n",
        "    sk_times[model_name] = time.time() - time_start_sk\n",
        "    print(f\"scikit-learn GridSearchCV training time ({model_name}) : {sk_times[model_name]:.2f} seconds\")\n",
        "    print(f\"scikit-learn Best parameters ({model_name}): {sk_models[model_name].best_params_}\")\n",
        "else:\n",
        "    sk_times[model_name] = 0\n",
        "    print(f\"Skipping scikit-learn {model_name} fitting as X_train or y_train is not defined.\")\n",
        "\n",
        "# 2) cuML KNN with the same GridSearchCV (adapted for cuML)\n",
        "if has_cuml and cuml_knn_available:\n",
        "    cu_models[model_name] = GridSearchCV(\n",
        "        estimator=cuMLKNeighborsClassifier(), \n",
        "        param_grid=param_grid_knn, \n",
        "        cv=5,\n",
        "        verbose=1,\n",
        "        n_jobs=1 \n",
        "    )\n",
        "    time_start_cu = time.time()\n",
        "    if X_train is not None and y_train is not None:\n",
        "        print(f\"Fitting cuML {model_name}...\")\n",
        "        cu_models[model_name].fit(X_train, y_train)\n",
        "        cu_times[model_name] = time.time() - time_start_cu\n",
        "        print(f\"cuml GridSearchCV training time ({model_name}) : {cu_times[model_name]:.2f} seconds\")\n",
        "        print(f\"cuml Best parameters ({model_name}): {cu_models[model_name].best_params_}\")\n",
        "    else:\n",
        "        cu_times[model_name] = 0\n",
        "        print(f\"Skipping cuML {model_name} fitting as X_train or y_train is not defined.\")\n",
        "elif has_cuml and not cuml_knn_available:\n",
        "    print(f\"cuML is available, but cuML {model_name} (KNeighborsClassifier) is not. Skipping cuML {model_name} benchmark.\")\n",
        "else:\n",
        "    print(f\"cuML is not installed or GPU not available. Skipping cuML {model_name} benchmark.\")\n",
        "\n",
        "if X_train is None or y_train is None:\n",
        "   print(f\"scikit-learn GridSearchCV training time ({model_name}) : N/A (X_train or y_train not defined)\")\n",
        "   print(f\"scikit-learn Best parameters ({model_name}): N/A (X_train or y_train not defined)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4127224",
      "metadata": {
        "id": "e4127224"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58222c34",
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1747242488819,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "58222c34"
      },
      "outputs": [],
      "source": [
        "model_name = \"RF\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a7ff61",
      "metadata": {
        "executionInfo": {
          "elapsed": 14,
          "status": "ok",
          "timestamp": 1747242488834,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "e4a7ff61"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as SklearnRandomForestClassifier\n",
        "# Attempt to import cuML's RandomForestClassifier\n",
        "try:\n",
        "    from cuml.ensemble import RandomForestClassifier as cuMLRandomForestClassifier\n",
        "    cuml_rf_available = True\n",
        "except ImportError:\n",
        "    cuml_rf_available = False\n",
        "    print(\"cuML RandomForestClassifier not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f713a30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 89411,
          "status": "ok",
          "timestamp": 1747242578246,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "3f713a30"
      },
      "outputs": [],
      "source": [
        "# Parameter grid for Random Forest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200], \n",
        "    'max_depth': [None, 10, 20],    \n",
        "    'min_samples_split': [2, 5], \n",
        "    'min_samples_leaf': [1, 2],   \n",
        "    'random_state': [42]\n",
        "}\n",
        "\n",
        "# 1) scikit-learn RandomForest with GridSearchCV\n",
        "sk_models[model_name] = GridSearchCV(\n",
        "    estimator=SklearnRandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid_rf,\n",
        "    cv=5, \n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "time_start_sk = time.time()\n",
        "if X_train is not None and y_train is not None:\n",
        "    print(f\"Fitting scikit-learn {model_name}...\")\n",
        "    sk_models[model_name].fit(X_train, y_train)\n",
        "    sk_times[model_name] = time.time() - time_start_sk\n",
        "    print(f\"scikit-learn GridSearchCV training time ({model_name}) : {sk_times[model_name]:.2f} seconds\")\n",
        "    print(f\"scikit-learn Best parameters ({model_name}): {sk_models[model_name].best_params_}\")\n",
        "else:\n",
        "    sk_times[model_name] = 0\n",
        "    print(f\"Skipping scikit-learn {model_name} fitting as X_train or y_train is not defined.\")\n",
        "\n",
        "\n",
        "# 2) cuML RandomForest with GridSearchCV (adapted for cuML)\n",
        "if has_cuml and cuml_rf_available:\n",
        "    # cuML RandomForestClassifier might have slightly different parameter names or accepted values.\n",
        "    # n_estimators, max_depth, min_samples_split, min_samples_leaf, random_state are generally compatible.\n",
        "    cu_models[model_name] = GridSearchCV(\n",
        "        estimator=cuMLRandomForestClassifier(random_state=42), \n",
        "        param_grid=param_grid_rf, \n",
        "        cv=5, \n",
        "        verbose=1,\n",
        "        n_jobs=1 \n",
        "    )\n",
        "\n",
        "    time_start_cu = time.time()\n",
        "    if X_train is not None and y_train is not None:\n",
        "        print(f\"Fitting cuML {model_name}...\")\n",
        "        cu_models[model_name].fit(X_train, y_train) \n",
        "        cu_times[model_name] = time.time() - time_start_cu\n",
        "        print(f\"cuml GridSearchCV training time ({model_name}) : {cu_times[model_name]:.2f} seconds\")\n",
        "        print(f\"cuml Best parameters ({model_name}): {cu_models[model_name].best_params_}\")\n",
        "    else:\n",
        "        cu_times[model_name] = 0\n",
        "        print(f\"Skipping cuML {model_name} fitting as X_train or y_train is not defined.\")\n",
        "elif has_cuml and not cuml_rf_available:\n",
        "    print(f\"cuML is available, but cuML {model_name} (RandomForestClassifier) is not. Skipping cuML {model_name} benchmark.\")\n",
        "else:\n",
        "    print(f\"cuML is not installed or GPU not available. Skipping cuML {model_name} benchmark.\")\n",
        "\n",
        "if X_train is None or y_train is None:\n",
        "   print(f\"scikit-learn GridSearchCV training time ({model_name}) : N/A (X_train or y_train not defined)\")\n",
        "   print(f\"scikit-learn Best parameters ({model_name}): N/A (X_train or y_train not defined)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XUmWcqLY7Xx4",
      "metadata": {
        "id": "XUmWcqLY7Xx4"
      },
      "source": [
        "# Prediction & Model Evaluation \n",
        "# (Commented out/Conditional as target 'y' is not defined for solar system data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01946225",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 2009,
          "status": "ok",
          "timestamp": 1747242580267,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "01946225"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Comparison and Evaluation ---\n",
        "model_names_fitted = list(sk_models.keys()) \n",
        "\n",
        "for model_name in model_names_fitted:\n",
        "    print(f\"--- Evaluating: {model_name} ---\")\n",
        "\n",
        "    # --- scikit-learn model ---\n",
        "    print(\"\\n-- scikit-learn --\")\n",
        "    if model_name in sk_models and hasattr(sk_models[model_name], 'best_estimator_') and y_train is not None and y_test is not None:\n",
        "        best_sk_model = sk_models[model_name].best_estimator_\n",
        "        # Predictions\n",
        "        y_pred_train_sk = best_sk_model.predict(X_train)\n",
        "        y_pred_test_sk = best_sk_model.predict(X_test)\n",
        "        start_time_pred_sk = time.time()\n",
        "        sk_pred[model_name] = best_sk_model.predict(X) # Predict on whole X for plotting\n",
        "        sk_pred_times[model_name] = time.time() - start_time_pred_sk\n",
        "        print(f\"Prediction Duration: {sk_pred_times[model_name]:.2f} seconds\")\n",
        "        # Accuracy\n",
        "        train_accuracy_sk = accuracy_score(y_train, y_pred_train_sk)\n",
        "        test_accuracy_sk = accuracy_score(y_test, y_pred_test_sk)\n",
        "        sk_train_accuracy[model_name] = train_accuracy_sk\n",
        "        sk_test_accuracy[model_name] = test_accuracy_sk\n",
        "        print(f\"Train Accuracy: {train_accuracy_sk:.4f}\")\n",
        "        print(f\"Test Accuracy: {test_accuracy_sk:.4f}\")\n",
        "        # Cross-validation score\n",
        "        cv_score_sk = sk_models[model_name].best_score_\n",
        "        sk_crossValidation_accuracy[model_name] = cv_score_sk\n",
        "        print(f\"Best Cross-Validation Score: {cv_score_sk:.4f}\")\n",
        "        # Duration\n",
        "        print(f\"Training Duration: {sk_times.get(model_name, 'N/A'):.2f} seconds\")\n",
        "        # Confusion Matrix\n",
        "        print(\"Confusion Matrix (Test Set):\")\n",
        "        cm_sk = confusion_matrix(y_test, y_pred_test_sk)\n",
        "        disp_sk = ConfusionMatrixDisplay(confusion_matrix=cm_sk, display_labels=best_sk_model.classes_)\n",
        "        disp_sk.plot()\n",
        "        plt.title(f\"scikit-learn {model_name} - Confusion Matrix\")\n",
        "        plt.show()\n",
        "    elif y_train is None or y_test is None:\n",
        "        print(f\"Skipping evaluation for scikit-learn {model_name} as y_train or y_test is not defined.\")\n",
        "        sk_pred[model_name] = None # Ensure key exists but is None\n",
        "        sk_pred_times[model_name] = 0\n",
        "    else:\n",
        "        print(f\"scikit-learn model {model_name} not trained or available for evaluation.\")\n",
        "        sk_pred[model_name] = None\n",
        "        sk_pred_times[model_name] = 0\n",
        "\n",
        "    # --- cuML model ---\n",
        "    print(\"\\n-- cuML --\")\n",
        "    if has_cuml and model_name in cu_models and hasattr(cu_models[model_name], 'best_estimator_') and y_train is not None and y_test is not None:\n",
        "        best_cu_model = cu_models[model_name].best_estimator_\n",
        "        # Predictions\n",
        "        X_train_np = X_train.to_numpy() if hasattr(X_train, 'to_numpy') else X_train\n",
        "        X_test_np = X_test.to_numpy() if hasattr(X_test, 'to_numpy') else X_test\n",
        "        X_np = X.to_numpy() if hasattr(X, 'to_numpy') else X\n",
        "\n",
        "        y_pred_train_cu_gpu = best_cu_model.predict(X_train_np)\n",
        "        y_pred_test_cu_gpu = best_cu_model.predict(X_test_np)\n",
        "        start_time_pred_cu = time.time()\n",
        "        cu_pred_gpu = best_cu_model.predict(X_np)\n",
        "        cu_pred_times[model_name] = time.time() - start_time_pred_cu\n",
        "        print(f\"Prediction Duration: {cu_pred_times[model_name]:.2f} seconds\")\n",
        "\n",
        "        y_pred_train_cu = y_pred_train_cu_gpu.get() if hasattr(y_pred_train_cu_gpu, 'get') else y_pred_train_cu_gpu\n",
        "        y_pred_test_cu = y_pred_test_cu_gpu.get() if hasattr(y_pred_test_cu_gpu, 'get') else y_pred_test_cu_gpu\n",
        "        cu_pred[model_name] = cu_pred_gpu.get() if hasattr(cu_pred_gpu, 'get') else cu_pred_gpu\n",
        "        \n",
        "        # Accuracy\n",
        "        train_accuracy_cu = accuracy_score(y_train, y_pred_train_cu) \n",
        "        test_accuracy_cu = accuracy_score(y_test, y_pred_test_cu)   \n",
        "        cu_train_accuracy[model_name] = train_accuracy_cu\n",
        "        cu_test_accuracy[model_name] = test_accuracy_cu\n",
        "        print(f\"Train Accuracy: {train_accuracy_cu:.4f}\")\n",
        "        print(f\"Test Accuracy: {test_accuracy_cu:.4f}\")\n",
        "        # Cross-validation score\n",
        "        cv_score_cu = cu_models[model_name].best_score_\n",
        "        # cu_crossValidation_accuracy[f\"cu_{model_name}\"] = cv_score_cu # Original had f-string key\n",
        "        cu_crossValidation_accuracy[model_name] = cv_score_cu # Corrected key\n",
        "        print(f\"Best Cross-Validation Score: {cv_score_cu:.4f}\")\n",
        "        # Duration\n",
        "        print(f\"Training Duration: {cu_times.get(model_name, 'N/A'):.2f} seconds\")\n",
        "        # Confusion Matrix\n",
        "        print(\"Confusion Matrix (Test Set):\")\n",
        "        cm_cu = confusion_matrix(y_test, y_pred_test_cu)\n",
        "        disp_cu = ConfusionMatrixDisplay(confusion_matrix=cm_cu, display_labels=best_cu_model.classes_)\n",
        "        disp_cu.plot()\n",
        "        plt.title(f\"cuML {model_name} - Confusion Matrix\")\n",
        "        plt.show()\n",
        "    elif has_cuml and (y_train is None or y_test is None):\n",
        "        print(f\"Skipping evaluation for cuML {model_name} as y_train or y_test is not defined.\")\n",
        "        cu_pred[model_name] = None\n",
        "        cu_pred_times[model_name] = 0\n",
        "    elif has_cuml:\n",
        "        print(f\"cuML model {model_name} not trained or available for evaluation.\")\n",
        "        cu_pred[model_name] = None\n",
        "        cu_pred_times[model_name] = 0\n",
        "    else:\n",
        "        print(f\"cuML {model_name} was not run as cuML is not available.\")\n",
        "        cu_pred[model_name] = None\n",
        "        cu_pred_times[model_name] = 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "print(\"\\nSummary of Accuracies and Times (if models were trained and y was defined):\")\n",
        "print(\"scikit-learn Train Accuracies:\", sk_train_accuracy)\n",
        "print(\"CuML Train Accuracies:\", cu_train_accuracy)\n",
        "print(\"scikit-learn Test Accuracies:\", sk_test_accuracy)\n",
        "print(\"CuML Test Accuracies:\", cu_test_accuracy)\n",
        "print(\"scikit-learn Cross-Validation Accuracies:\", sk_crossValidation_accuracy)\n",
        "print(\"CuML Cross-Validation Accuracies:\", cu_crossValidation_accuracy)\n",
        "print(\"scikit-learn Training Times:\", sk_times)\n",
        "print(\"cuML Training Times:\", cu_times)\n",
        "print(\"scikit-learn Prediction Times:\", sk_pred_times)\n",
        "print(\"cuML Prediction Times:\", cu_pred_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aacdfed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 4434,
          "status": "ok",
          "timestamp": 1747242584702,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "8aacdfed"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# This cell handles plotting. It will attempt to use a 'Date'/'Time' derived axis or fallback to index.\n",
        "# Classification tracks are commented out as 'y' is not defined for the solar system data.\n",
        "\n",
        "# TODO: Define a suitable 'y_axis_plot' and 'y_axis_label' for the new dataset.\n",
        "# Example: If 'Date' and 'Time' columns exist and can form a datetime index:\n",
        "if not generate_mock_data and 'Date' in data.columns and 'Time' in data.columns:\n",
        "    try:\n",
        "        # Attempt to parse Date and Time into a single datetime series\n",
        "        datetime_series = pd.to_datetime(data['Date'] + ' ' + data['Time'], errors='coerce')\n",
        "        data_for_plot = data.loc[datetime_series.sort_values().index].copy() # Use .copy() for a new DataFrame\n",
        "        X_for_plot = X.loc[datetime_series.sort_values().index].copy()\n",
        "        # For plotting, a simple numerical sequence based on sorted time might be best if time intervals are not uniform\n",
        "        y_axis_plot = np.arange(len(data_for_plot))\n",
        "        y_axis_label = \"Time Sequence Index\"\n",
        "        print(\"Using Time Sequence Index for y-axis of plots.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not create datetime series from 'Date' and 'Time': {e}. Using DataFrame index for y-axis.\")\n",
        "        data_for_plot = data.copy()\n",
        "        X_for_plot = X.copy()\n",
        "        y_axis_plot = data_for_plot.index # Fallback to index\n",
        "        y_axis_label = \"Index\"\n",
        "elif generate_mock_data and 'Y_AXIS_MOCK' in data.columns:\n",
        "    data_for_plot = data.copy()\n",
        "    X_for_plot = X.copy()\n",
        "    y_axis_plot = data_for_plot['Y_AXIS_MOCK']\n",
        "    y_axis_label = \"Mock Y Axis\"\n",
        "    print(\"Using Mock Y Axis for y-axis of plots.\")\n",
        "else:\n",
        "    print(\"Warning: 'Date' and 'Time' columns not found or using mock data without Y_AXIS_MOCK. Using DataFrame index for y-axis of feature tracks.\")\n",
        "    data_for_plot = data.copy()\n",
        "    X_for_plot = X.copy()\n",
        "    y_axis_plot = data_for_plot.index # Fallback to index\n",
        "    y_axis_label = \"Index\"\n",
        "\n",
        "feature_names_plot = X_for_plot.columns.tolist()\n",
        "\n",
        "# --- Figure 1: Feature Tracks ---\n",
        "num_feature_plots = len(feature_names_plot)\n",
        "if num_feature_plots > 0:\n",
        "    fig_features, axes_features = plt.subplots(1, num_feature_plots, figsize=(num_feature_plots * 2.5, 10), sharey=True)\n",
        "    if num_feature_plots == 1: axes_features = [axes_features] # Ensure axes_features is always a list\n",
        "    fig_features.suptitle(\"Feature Tracks\", fontsize=16, y=0.98)\n",
        "\n",
        "    for i, feature_name in enumerate(feature_names_plot):\n",
        "        ax = axes_features[i]\n",
        "        ax.plot(data_for_plot[feature_name], y_axis_plot)\n",
        "        ax.set_title(feature_name, fontsize=10)\n",
        "        ax.set_xlabel(\"Value\", fontsize=8)\n",
        "        if i == 0:\n",
        "            ax.set_ylabel(y_axis_label, fontsize=10)\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax.tick_params(axis='x', labelsize=8)\n",
        "        ax.tick_params(axis='y', labelsize=8)\n",
        "\n",
        "    if num_feature_plots > 0 : axes_features[0].invert_yaxis() # Invert if it makes sense (e.g. time sequence or depth)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No features to plot for feature tracks.\")\n",
        "\n",
        "# --- Figure 2: Scaled Feature Tracks ---\n",
        "if num_feature_plots > 0:\n",
        "    fig_scaled_features, axes_scaled_features = plt.subplots(1, num_feature_plots, figsize=(num_feature_plots * 2.5, 10), sharey=True)\n",
        "    if num_feature_plots == 1: axes_scaled_features = [axes_scaled_features]\n",
        "    fig_scaled_features.suptitle(\"Scaled Feature Tracks (After Quantile Transformation)\", fontsize=16, y=0.98)\n",
        "\n",
        "    for i, feature_name in enumerate(feature_names_plot):\n",
        "        ax = axes_scaled_features[i]\n",
        "        ax.plot(X_for_plot[feature_name], y_axis_plot)\n",
        "        ax.set_title(feature_name, fontsize=10)\n",
        "        ax.set_xlabel(\"Value\", fontsize=8)\n",
        "        if i == 0:\n",
        "            ax.set_ylabel(y_axis_label, fontsize=10)\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax.tick_params(axis='x', labelsize=8)\n",
        "        ax.tick_params(axis='y', labelsize=8)\n",
        "\n",
        "    if num_feature_plots > 0 : axes_scaled_features[0].invert_yaxis()\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No features to plot for scaled feature tracks.\")\n",
        "\n",
        "# --- Figure 3: Classification Tracks (Commented out) ---\n",
        "print(\"\\nClassification Tracks plotting is skipped as 'y' is not defined for the solar system data or task is not classification.\")\n",
        "# if y is not None and not y.empty: # Check if y is defined and not empty\n",
        "#     all_predictions_to_plot = {}\n",
        "#     all_predictions_to_plot['True Labels'] = y.loc[X_for_plot.index] # Align with X_for_plot for consistent y-axis\n",
        "#     for model_name, preds_array in sk_pred.items():\n",
        "#         if preds_array is not None:\n",
        "#             all_predictions_to_plot[f'SK: {model_name}'] = pd.Series(preds_array, index=X.index).loc[X_for_plot.index]\n",
        "#     if has_cuml and cu_pred:\n",
        "#         for model_name, preds_cu_array in cu_pred.items():\n",
        "#             if preds_cu_array is not None:\n",
        "#                 all_predictions_to_plot[f'CU: {model_name}'] = pd.Series(preds_cu_array, index=X.index).loc[X_for_plot.index]\n",
        "#\n",
        "#     num_classification_plots = len(all_predictions_to_plot)\n",
        "#     if num_classification_plots > 0:\n",
        "#         fig_class, axes_class = plt.subplots(1, num_classification_plots, figsize=(num_classification_plots * 1.2, 10), sharey=True)\n",
        "#         if num_classification_plots == 1: axes_class = [axes_class]\n",
        "#         fig_class.suptitle(\"Classification Tracks\", fontsize=16, y=0.98)\n",
        "#\n",
        "#         plot_order = ['True Labels'] + [k for k in all_predictions_to_plot.keys() if k != 'True Labels']\n",
        "#\n",
        "#         unique_classes = np.sort(y.unique())\n",
        "#         if len(unique_classes) == 1: plot_colors = ['gold']\n",
        "#         elif len(unique_classes) == 2: plot_colors = ['gold', 'darkorange']\n",
        "#         else: \n",
        "#             cmap_tab10 = plt.cm.get_cmap('tab10', len(unique_classes))\n",
        "#             plot_colors = [mcolors.to_hex(cmap_tab10(i)) for i in range(len(unique_classes))]\n",
        "#         class_to_int = {cls: i for i, cls in enumerate(unique_classes)}\n",
        "#         cmap_listed = mcolors.ListedColormap(plot_colors[:len(unique_classes)])\n",
        "#\n",
        "#         for i, title in enumerate(plot_order):\n",
        "#             predictions_data = all_predictions_to_plot[title]\n",
        "#             ax = axes_class[i]\n",
        "#             mapped_preds = predictions_data.map(class_to_int).fillna(-1) # Use aligned predictions_data\n",
        "#             labels_int = mapped_preds.values.reshape(-1, 1)\n",
        "#             vmin = 0\n",
        "#             vmax = len(plot_colors) - 1\n",
        "#\n",
        "#             ax.imshow(labels_int, aspect='auto', cmap=cmap_listed,\n",
        "#                       extent=[0, 1, y_axis_plot.max(), y_axis_plot.min()], # Use y_axis_plot for extent\n",
        "#                       interpolation='none', vmin=vmin, vmax=vmax)\n",
        "#\n",
        "#             ax.set_title(title, fontsize=10)\n",
        "#             ax.set_xticks([])\n",
        "#             ax.set_xlabel(\"\")\n",
        "#             ax.tick_params(axis='y', labelsize=8)\n",
        "#             if i == 0: ax.set_ylabel(y_axis_label, fontsize=10)\n",
        "#\n",
        "#         if num_classification_plots > 0: axes_class[0].invert_yaxis()\n",
        "#\n",
        "#         if len(unique_classes) > 0:\n",
        "#             patches = [plt.Rectangle((0,0),1,1, color=plot_colors[class_to_int[cls]]) for cls in unique_classes]\n",
        "#             legend_labels = [str(cls) for cls in unique_classes]\n",
        "#             fig_class.legend(patches, legend_labels, loc='lower center', ncol=len(unique_classes), bbox_to_anchor=(0.5, -0.02), title=\"Classes\")\n",
        "#\n",
        "#         plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
        "#         plt.show()\n",
        "# else:\n",
        "#     print(\"No classification data to plot (y is None or empty).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a42055e8",
      "metadata": {
        "id": "a42055e8"
      },
      "source": [
        "# Shutdown colab runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f7b98c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1747242584708,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "65f7b98c"
      },
      "outputs": [],
      "source": [
        "# Execution time\n",
        "end_notebook = time.time()\n",
        "print(f\"Total notebook execution time: {end_notebook - start_notebook:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0l5k3AxE-IlM",
      "metadata": {
        "executionInfo": {
          "elapsed": 384,
          "status": "ok",
          "timestamp": 1747242585092,
          "user": {
            "displayName": "2065_Ahmad Royyan Fatah",
            "userId": "06797199535233841376"
          },
          "user_tz": -420
        },
        "id": "0l5k3AxE-IlM"
      },
      "outputs": [],
      "source": [
        "if is_running_on_colab:\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
